(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{253:function(e,t,n){"use strict";n.r(t);var a=n(0),i=Object(a.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"feature-projection-in-deep-neural-networks"}},[e._v("Feature Projection in Deep Neural Networks")]),e._v(" "),n("p",[e._v("时间：3月20日周二下午3:00")]),e._v(" "),n("p",[e._v("地点：电院1号楼518A")]),e._v(" "),n("h3",{attrs:{id:"摘要："}},[e._v("摘要：")]),e._v(" "),n("p",[e._v('It is commonly believed that the hidden layers of deep neural networks (DNN) attempt to extract informative features for learning tasks. In this talk, we make this intuition precise by showing that the features extracted by DNN coincide with the result of an optimization problem, which we call the "universal feature selection" problem. We interpret the learning in DNN as projection of feature functions onto functional subspaces, specified by the network structure. Our formulation has several advantages: 1) it has direct operational meaning in terms of the performance for inference tasks; 2) it can be solved by alternative numerical procedures and sometimes even analytically, providing rich alternatives to DNN implementations; 3) it gives interpretations to the internal computation results of DNN; and 4) it naturally leads to a performance metric that can be applied to general DNN as well as other feature selection algorithms, which can be used to evaluate the effectiveness of the DNN\n  structures and improve designs. We present the results of some numerical experiments to support our approach.')]),e._v(" "),n("h3",{attrs:{id:"简介："}},[e._v("简介：")]),e._v(" "),n("p",[e._v("Shao-Lun Huang received the B.S. degree with honor in 2008 from the Department of Electronic Engineering, National Taiwan University, Taiwan, and the M.S. and Ph.D. degree in 2010 and 2013 from the Department of Electronic Engineering and Computer Sciences, Massachusetts Institute of Technology. From 2013 to 2016, he was working as a postdoctoral researcher jointly in the Department of Electrical Engineering at the National Taiwan University and the Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology. Since 2016, he has joined Tsinghua-Berkeley Shenzhen Institute, where he is currently an assistant professor. His research interests include information theory, communication theory, machine learning, and social networks.")])])}],!1,null,null,null);t.default=i.exports}}]);